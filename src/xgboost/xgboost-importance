import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import time
import pandas as pd
# model libraries
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
# data libraries
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# terminal libraries, logger
from rich.logging import RichHandler
from rich.progress import track
import logging

FORMAT = "%(message)s"
logging.basicConfig(
    level="NOTSET", format=FORMAT, datefmt="[%X]", handlers=[RichHandler()]
)

log = logging.getLogger("rich")
log.info("Starting the program ...")

# set log level
log.setLevel(logging.INFO)

# -----------------------------------------
# 1. FUNCTIONS FOR TRAINING AND EVALUATION

# dataset import function
def load_single_csv(path):
    df = pd.read_csv(path)
    log.info(f"Loaded {path}.")
    return df

# split dataset into X and y
def split_dataset(df, target):
    X = df.drop(target, axis=1)
    y = df[target]
    log.info(f"Split dataset into X and y.")
    return X, y

# split dataset into train and test, 20% test size
def split_train_test(X, y):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2
    )
    log.info(f"Split dataset into train and test.")
    return X_train, X_test, y_train, y_test

# train XGBoost model
def train_xgb(X_train, y_train, X_test, y_test):
    model = XGBClassifier()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    log.info(f"Trained XGBoost model.")
    return y_pred, model

# train LightGBM model
def train_lgb(X_train, y_train, X_test, y_test):
    model = LGBMClassifier()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    log.info(f"Trained LightGBM model.")
    return y_pred, model

# k fold cross validation for XGBoost sving the best model
def k_fold_xgboost(X, y, k=5):
    start = time.time()
    best_model = None
    best_score = 0
    best_pred = None
    for i in track(range(k), description=f"Cross validation iterating for XGBoost"):
        X_train, X_test, y_train, y_test = split_train_test(X, y)
        start_iter = time.time()
        y_pred, model = train_xgb(X_train, y_train, X_test, y_test)
        score = accuracy_score(y_test, y_pred)
        end_iter = time.time()
        log.info(f"Cross validation {i} iteration took {end_iter - start_iter} seconds.")
        if score > best_score:
            best_score = score
            best_model = model
            best_pred = y_pred
            best_y_test = y_test
            best_X_test = X_test
    end = time.time()
    log.info(f"Cross validation {i} took {end - start} seconds.")
    log.info(f"Best score: {best_score}")
    return best_model, best_pred, best_y_test, best_X_test

# k fold cross validation for LightGBM using the best model
def k_fold_lightgbm(X, y, k=5):
    start = time.time()
    best_model = None
    best_score = 0
    best_pred = None    

    for i in track(range(k), description=f"Cross validation iterating for LightGBM"):
        X_train, X_test, y_train, y_test = split_train_test(X, y)
        start_iter = time.time()
        y_pred, model = train_lgb(X_train, y_train, X_test, y_test)
        score = accuracy_score(y_test, y_pred)
        end_iter = time.time()
        log.info(f"Cross validation {i} iteration took {end_iter - start_iter} seconds.")
        if score > best_score:
            best_score = score
            best_model = model
            best_pred = y_pred
            best_y_test = y_test
            best_X_test = X_test
    end = time.time()
    log.info(f"Cross validation {i} took {end - start} seconds.")
    log.info(f"Best score: {best_score}")
    return best_model, best_pred, best_y_test, best_X_test




# -----------------------------------------
# 2. FUNCTIONS FOR FEATURE IMPORTANCE

# permutation importance
# permutation importance is a model inspection technique that can be used 
# for any fitted estimator when the data is tabular. 
# This is especially useful for non-linear or opaque estimators, and works by
# shuffling each feature and measuring the loss in accuracy.
def permutation_check(model, X_test, y_test):
    start = time.time()
    perm = permutation_importance(
        model, X_test, y_test, n_repeats=10, random_state=42
    )
    end = time.time()
    log.info(f"Permutation importance took {end - start} seconds.")
    return perm

# feature importance
# feature importance is calculated by measuring the increase in the model’s
# prediction error after permuting the feature’s values, which breaks the
# relationship between the feature and the true outcome.
def feature_check(model):
    start = time.time()
    importance = model.feature_importances_
    end = time.time()
    log.info(f"Feature importance took {end - start} seconds.")
    return importance


# -----------------------------------------
# 3. FUNCTIONS FOR VISUALIZATION

# plot permutation importance
def plot_permutation_importance(X_test, perm):
    start = time.time()
    sorted_idx = perm.importances_mean.argsort()
    fig, ax = plt.figure(figsize=(10, 100)), plt.axes()
    ax.boxplot(
        perm.importances[sorted_idx].T,
        vert=False,
        labels=X_test.columns[sorted_idx],
    )
    ax.set_title("Permutation Importances (test set)")
    ax.autoscale(enable=True, axis='x', tight=True)  # Auto-range x-axis
    fig.tight_layout()
    end = time.time()
    log.info(f"Plot permutation importance took {end - start} seconds.")
    return fig, ax

# plot feature importance
def plot_feature_importance(X_test, importance):
    start = time.time()
    fig, ax = plt.figure(figsize=(10, 100)), plt.axes()
    ax.barh(X_test.columns, importance)
    ax.set_title("Feature Importances (test set)")
    fig.tight_layout()
    end = time.time()
    log.info(f"Plot feature importance took {end - start} seconds.")
    return fig, ax

# plot confusion matrix
def plot_confusion_matrix(y_test, y_pred):
    start = time.time()
    fig, ax = plt.subplots()
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(ax=ax)
    ax.set_title("Confusion Matrix")
    fig.tight_layout()
    end = time.time()
    log.info(f"Plot confusion matrix took {end - start} seconds.")
    return fig, ax

# plot actual vs predicted
def plot_actual_vs_predicted(y_test, y_pred):
    start = time.time()
    fig, ax = plt.subplots()
    ax.scatter(y_test, y_pred)
    ax.set_xlabel("Actual")
    ax.set_ylabel("Predicted")
    ax.set_title("Actual vs Predicted")
    fig.tight_layout()
    end = time.time()
    log.info(f"Plot actual vs predicted took {end - start} seconds.")
    return fig, ax


# -----------------------------------------
# 4. MAIN FUNCTION

def main():

    path = "data/processed_data/subject_010.csv"

    # load dataset
    df = load_single_csv(path)
    # drop timestamp column
    df = df.drop("ts", axis=1)
    # split dataset into X and y
    X, y = split_dataset(df, target="grasp")
    log.info(f"Dataset loaded and split into X and y.")

    # train XGBoost model
    model, y_pred, y_test, X_test = k_fold_xgboost(X, y)
    log.info(f"Trained XGBoost model.")
    # plot confusion matrix for XGBoost
    fig, ax = plot_confusion_matrix(y_test, y_pred)
    fig.savefig("plots/confusion_matrix_xgboost.png")
    log.info(f"Confusion matrix saved to plots/confusion_matrix_xgboost.png")
    # plot actual vs predicted for XGBoost
    fig, ax = plot_actual_vs_predicted(y_test, y_pred)
    fig.savefig("plots/actual_vs_predicted_xgboost.png")
    log.info(f"Actual vs predicted saved to plots/actual_vs_predicted_xgboost.png")
    # plot feature importance for XGBoost
    importance = feature_check(model)
    fig, ax = plot_feature_importance(X_test, importance)
    fig.savefig("plots/feature_importance_xgboost.png")
    log.info(f"Feature importance saved to plots/feature_importance_xgboost.png")
    # plot permutation importance for XGBoost
    perm = permutation_check(model, X_test, y_test)
    fig, ax = plot_permutation_importance(X_test, perm)
    fig.savefig("plots/permutation_importance_xgboost.png")
    log.info(f"Permutation importance saved to plots/permutation_importance_xgboost.png")


    # train LightGBM model
    model, y_pred, y_test, X_test = k_fold_lightgbm(X, y)
    log.info(f"Trained LightGBM model.")
    # plot confusion matrix for LightGBM
    fig, ax = plot_confusion_matrix(y_test, y_pred)
    fig.savefig("plots/confusion_matrix_lightgbm.png")
    log.info(f"Confusion matrix saved to plots/confusion_matrix_lightgbm.png")
    # plot actual vs predicted for LightGBM
    fig, ax = plot_actual_vs_predicted(y_test, y_pred)
    fig.savefig("plots/actual_vs_predicted_lightgbm.png")
    log.info(f"Actual vs predicted saved to plots/actual_vs_predicted_lightgbm.png")
    # plot feature importance for LightGBM
    importance = feature_check(model)
    fig, ax = plot_feature_importance(X_test, importance)
    fig.savefig("plots/feature_importance_lightgbm.png")
    log.info(f"Feature importance saved to plots/feature_importance_lightgbm.png")
    # plot permutation importance for LightGBM
    perm = permutation_check(model, X_test, y_test)
    fig, ax = plot_permutation_importance(X_test, perm)
    fig.savefig("plots/permutation_importance_lightgbm.png")
    log.info(f"Permutation importance saved to plots/permutation_importance_lightgbm.png")

    return None


# run main function
if __name__ == "__main__":
    main()
    # once the program is finished, print the following message
    log.info("Program finished.")
    # exit the program
    exit(0)