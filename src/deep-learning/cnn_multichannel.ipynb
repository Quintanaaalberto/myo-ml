{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Multichannel\n",
    "A notebook to study the importance of features from sEMG measurements when explaining hand movements. Explained through the use of CNN and Saliency maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch will be our main engine for this project\n",
    "import torch\n",
    "\n",
    "# helper library for sEMG processing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "\n",
    "# visualisation tools\n",
    "import torchvision\n",
    "from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "seed_value = 201700448\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import torch\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.0040e-07,  1.4964e-06, -1.4122e-05,  ...,  8.9008e-07,\n",
      "          6.5215e-06,  1.0000e+00],\n",
      "        [ 1.7071e-07,  3.3427e-06, -1.3787e-05,  ...,  2.5688e-06,\n",
      "          4.7953e-06,  1.0000e+00],\n",
      "        [ 8.4182e-07,  4.0140e-06, -1.2612e-05,  ..., -1.1656e-07,\n",
      "          1.6840e-06,  1.0000e+00],\n",
      "        ...,\n",
      "        [ 7.5726e-07, -1.7326e-06, -5.8029e-06,  ..., -4.2426e-06,\n",
      "         -1.3934e-06,  0.0000e+00],\n",
      "        [ 2.6038e-06,  2.7994e-06, -3.4529e-06,  ..., -4.5785e-06,\n",
      "         -1.8521e-06,  0.0000e+00],\n",
      "        [ 3.2752e-06,  7.3314e-06,  1.2470e-06,  ..., -2.3966e-06,\n",
      "          5.1100e-08,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "ds = torch.load('dataset_10-19.pt')\n",
    "# explain the dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: prepare the data for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 70271699\n",
      "Test: 8783962\n",
      "Validation: 8783963\n"
     ]
    }
   ],
   "source": [
    "# split the data into train, test and validation\n",
    "train_size = int(0.8 * len(ds))\n",
    "test_size = int(0.1 * len(ds))\n",
    "val_size = len(ds) - train_size - test_size\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = torch.utils.data.random_split(ds, [train_size, test_size, val_size])\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Test: {len(test_dataset)}\")\n",
    "print(f\"Validation: {len(val_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class sEMGDataset(Dataset):\n",
    "    def __init__(self, ds):\n",
    "        # Separate the sEMG data from the labels\n",
    "        self.data = ds[:, :12]  # First 12 columns are sEMG data\n",
    "        self.labels = ds[:, 12].long()  # Last column is the label, converted to long for CrossEntropyLoss\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the sEMG data sample and reshape it to 12x1\n",
    "        sample = self.data[idx].view(12,1)  # Reshape to 12 channels, 1 time step\n",
    "        # we do this because the Conv1d layer in PyTorch expects the input to be in the format (batch, channels, time) \n",
    "        # Get the label\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "    \n",
    "    def to(self, mps_device):\n",
    "        self.data = self.data.to(mps_device)\n",
    "        self.labels = self.labels.to(mps_device)\n",
    "        print(\"Dataset moved to\", mps_device)\n",
    "\n",
    "\n",
    "class DataLoaderX(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(DataLoaderX, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def to(self, mps_device):\n",
    "        for batch_idx, (data, target) in enumerate(self):\n",
    "            self.data[batch_idx] = data.to(mps_device)\n",
    "            self.target[batch_idx] = target.to(mps_device)\n",
    "        print(\"DataLoader moved to\", mps_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the CNN model which will be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Conv1d_layer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        '''\n",
    "        in_channels: number of input channels\n",
    "        out_channels: number of output channels\n",
    "        kernel_size: size of the kernel\n",
    "        stride: stride of the kernel\n",
    "        padding: padding of the kernel\n",
    "        '''\n",
    "        super(Conv1d_layer, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.BatchNorm = nn.BatchNorm1d(out_channels)\n",
    "        self.MaxPool1d = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        x = self.BatchNorm(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.MaxPool1d(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv1d_Net(nn.Module):\n",
    "    def __init__(self, input_shape, kernel_size=3, stride=1, padding=1, out_channels=11):\n",
    "        super(Conv1d_Net, self).__init__()\n",
    "        in_channels = input_shape[1]  # Assuming input_shape is [channels, length]\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.conv3 = nn.Conv1d(out_channels, out_channels, kernel_size, stride, padding)\n",
    "        # Removed reference to conv4 since it's not defined\n",
    "        # Calculate the size of the output from conv layers dynamically\n",
    "        self.fc1 = nn.Linear(self._get_conv_output(input_shape), 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        # Removed softmax for reasons mentioned above\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return x\n",
    "    \n",
    "    def _get_conv_output(self, shape):\n",
    "        with torch.no_grad():  # Temporarily set all requires_grad flags to False\n",
    "            input = torch.autograd.Variable(torch.rand(1, *shape))  # Dummy input\n",
    "            output_feat = self._forward_features(input)\n",
    "            n_size = output_feat.data.view(1, -1).size(1)\n",
    "        return n_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the FC layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Return logits directly if using nn.CrossEntropyLoss\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = sEMGDataset(train_dataset.dataset)\n",
    "dl_train = DataLoaderX(ds_train, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([87839624, 12])\n"
     ]
    }
   ],
   "source": [
    "# plot dataset shape\n",
    "print(ds_train.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 12, 1])\n"
     ]
    }
   ],
   "source": [
    "# plot dataloader first batch shape\n",
    "print(next(iter(dl_train))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [11, 1, 3], expected input[1, 12, 1] to have 1 channels, but got 12 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m sample_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# This excludes the batch dimension\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create an instance of the model with the correct input shape\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mConv1d_Net\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Use torchviz to visualize the model\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# The model(input) call should be wrapped in torchviz.make_dot\u001b[39;00m\n\u001b[1;32m     23\u001b[0m model_output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m)  \u001b[38;5;66;03m# Forward pass with the corrected input\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 14\u001b[0m, in \u001b[0;36mConv1d_Net.__init__\u001b[0;34m(self, input_shape, kernel_size, stride, padding, out_channels)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv1d(out_channels, out_channels, kernel_size, stride, padding)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Removed reference to conv4 since it's not defined\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate the size of the output from conv layers dynamically\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_conv_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 27\u001b[0m, in \u001b[0;36mConv1d_Net._get_conv_output\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Temporarily set all requires_grad flags to False\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mVariable(torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mshape))  \u001b[38;5;66;03m# Dummy input\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     output_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     n_size \u001b[38;5;241m=\u001b[39m output_feat\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_size\n",
      "Cell \u001b[0;32mIn[33], line 19\u001b[0m, in \u001b[0;36mConv1d_Net._forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))\n",
      "File \u001b[0;32m~/miniforge3/envs/acc-pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/acc-pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/acc-pytorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/acc-pytorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [11, 1, 3], expected input[1, 12, 1] to have 1 channels, but got 12 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchviz\n",
    "from torch.nn import Conv1d, Linear, ReLU, Softmax  # Ensure you have the correct imports for your model\n",
    "\n",
    "# Assuming Conv1d_Net is defined as per previous discussions\n",
    "# Assuming dl_train is your DataLoader\n",
    "\n",
    "device = 'cpu'\n",
    "# Get the first batch from the DataLoader and prepare a single sample's shape\n",
    "input, _ = next(iter(dl_train))  # Gets the first batch (input and labels)\n",
    "input = input.to(device)  # Ensure input is on the right device\n",
    "\n",
    "# The model expects the input shape without the batch size\n",
    "# For Conv1d, the input shape should be (channels, length), excluding the batch size\n",
    "# Since your DataLoader batch is shaped as [batch_size, channels, length], use one sample's shape for model initialization\n",
    "sample_shape = input.shape[1:]  # This excludes the batch dimension\n",
    "\n",
    "# Create an instance of the model with the correct input shape\n",
    "model = Conv1d_Net(input_shape=sample_shape).to(device)\n",
    "\n",
    "# Use torchviz to visualize the model\n",
    "# The model(input) call should be wrapped in torchviz.make_dot\n",
    "model_output = model(input)  # Forward pass with the corrected input\n",
    "visual_graph = torchviz.make_dot(model_output, params=dict(model.named_parameters()))\n",
    "\n",
    "# To render and view the graph, depending on your environment, you might directly display visual_graph\n",
    "# or save it to a file. For example:\n",
    "visual_graph.render(\"model_visualization\", format=\"png\", directory=\"/mnt/data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Define the dataset and the dataloader, and draw the diagram of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device agnostic code (will run on GPU if available, otherwise CPU)\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset moved to mps\n",
      "Train shape:  torch.Size([87839624, 12])\n"
     ]
    }
   ],
   "source": [
    "ds_train.to(torch.device(device = device))\n",
    "print(\"Train shape: \", ds_train.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: Start train/validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader\n",
    "train_loader = DataLoaderX(train_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoaderX(val_dataset, batch_size=32, shuffle=False)  # No need to shuffle the validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and loss function\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader)):\n\u001b[0;32m---> 17\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     18\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acc-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
